{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset trained in kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from random import *\n",
    "from PIL import Image\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, ELU, Activation, BatchNormalization\n",
    "from keras.layers.convolutional import Convolution2D, Cropping2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "d = {}\n",
    "from subprocess import check_output\n",
    "# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "# forms = pd.read_csv('../input/iam-handwriting-top50/forms_for_parsing.txt', header=None)\n",
    "# print(forms.head)\n",
    "with open('/kaggle/input/iam-handwriting-top50/forms_for_parsing.txt') as f:\n",
    "    for line in f:\n",
    "        key = line.split(' ')[0]\n",
    "        writer = line.split(' ')[1]\n",
    "        d[key] = writer\n",
    "#print(len(d.keys()))\n",
    "\n",
    "tmp = []\n",
    "target_list = []\n",
    "\n",
    "path_to_files = os.path.join('/kaggle/input/iam-handwriting-top50/data_subset/data_subset', '*')\n",
    "for filename in sorted(glob.glob(path_to_files)):\n",
    "#     print(filename)\n",
    "    tmp.append(filename)\n",
    "    image_name = filename.split('/')[-1]\n",
    "    file, ext = os.path.splitext(image_name)\n",
    "    parts = file.split('-')\n",
    "    form = parts[0] + '-' + parts[1]\n",
    "    for key in d:\n",
    "        if key == form:\n",
    "            target_list.append(str(d[form]))\n",
    "\n",
    "img_files = np.asarray(tmp)\n",
    "img_targets = np.asarray(target_list)\n",
    "print(img_files.shape)\n",
    "print(img_targets.shape)\n",
    "\n",
    "'''for filename in img_files[:3]:\n",
    "    img=mpimg.imread(filename)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(img, cmap ='gray')'''\n",
    "    \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(img_targets)\n",
    "encoded_Y = encoder.transform(img_targets)\n",
    "\n",
    "#print(img_files[:5], img_targets[:5], encoded_Y[:5]) \n",
    "\n",
    "train_files, rem_files, train_targets, rem_targets = train_test_split(\n",
    "        img_files, encoded_Y, train_size=0.66, random_state=52, shuffle= True)\n",
    "\n",
    "validation_files, test_files, validation_targets, test_targets = train_test_split(\n",
    "        rem_files, rem_targets, train_size=0.5, random_state=22, shuffle=True)\n",
    "\n",
    "print(train_files.shape, validation_files.shape, test_files.shape)\n",
    "print(train_targets.shape, validation_targets.shape, test_targets.shape)\n",
    "\n",
    "\n",
    "batch_size = 8 #16\n",
    "num_classes = 50\n",
    "\n",
    "# Start with train generator shared in the class and add image augmentations\n",
    "def generate_data(samples, target_files,  batch_size=batch_size, factor = 0.1 ):\n",
    "    num_samples = len(samples)\n",
    "    from sklearn.utils import shuffle\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            batch_targets = target_files[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            targets = []\n",
    "            for i in range(len(batch_samples)):\n",
    "                batch_sample = batch_samples[i]\n",
    "                batch_target = batch_targets[i]\n",
    "                im = Image.open(batch_sample)\n",
    "                cur_width = im.size[0]\n",
    "                cur_height = im.size[1]\n",
    "\n",
    "                # print(cur_width, cur_height)\n",
    "                height_fac = 113 / cur_height\n",
    "\n",
    "                new_width = int(cur_width * height_fac)\n",
    "                size = new_width, 113\n",
    "\n",
    "                imresize = im.resize((size), Image.ANTIALIAS)  # Resize so height = 113 while keeping aspect ratio\n",
    "                now_width = imresize.size[0]\n",
    "                now_height = imresize.size[1]\n",
    "                # Generate crops of size 113x113 from this resized image and keep random 10% of crops\n",
    "\n",
    "                avail_x_points = list(range(0, now_width - 113 ))# total x start points are from 0 to width -113\n",
    "\n",
    "                # Pick random x%\n",
    "                pick_num = int(len(avail_x_points)*factor)\n",
    "\n",
    "                # Now pick\n",
    "                random_startx = sample(avail_x_points,  pick_num)\n",
    "\n",
    "                for start in random_startx:\n",
    "                    imcrop = imresize.crop((start, 0, start+113, 113))\n",
    "                    images.append(np.asarray(imcrop))\n",
    "                    targets.append(batch_target)\n",
    "\n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(targets)\n",
    "\n",
    "            #reshape X_train for feeding in later\n",
    "            X_train = X_train.reshape(X_train.shape[0], 113, 113, 1)\n",
    "            #convert to float and normalize\n",
    "            X_train = X_train.astype('float32')\n",
    "            X_train /= 255\n",
    "\n",
    "            #One hot encode y\n",
    "            y_train = to_categorical(y_train, num_classes)\n",
    "            yield shuffle(X_train, y_train)\n",
    "            \n",
    "train_generator = generate_data(train_files, train_targets, batch_size=batch_size, factor = 0.3)\n",
    "validation_generator = generate_data(validation_files, validation_targets, batch_size=batch_size, factor = 0.3)\n",
    "test_generator = generate_data(test_files, test_targets, batch_size=batch_size, factor = 0.1)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, rem_files, train_targets, rem_targets = train_test_split(\n",
    "        img_files, encoded_Y, train_size=0.66, random_state=52, shuffle= True)\n",
    "\n",
    "validation_files, test_files, validation_targets, test_targets = train_test_split(\n",
    "        rem_files, rem_targets, train_size=0.5, random_state=22, shuffle=True)\n",
    "\n",
    "print(train_files.shape, validation_files.shape, test_files.shape)\n",
    "print(train_targets.shape, validation_targets.shape, test_targets.shape)\n",
    "\n",
    "\n",
    "batch_size = 8 #16\n",
    "num_classes = 50\n",
    "\n",
    "# Start with train generator shared in the class and add image augmentations\n",
    "def generate_data(samples, target_files,  batch_size=batch_size, factor = 0.1 ):\n",
    "    num_samples = len(samples)\n",
    "    from sklearn.utils import shuffle\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            batch_targets = target_files[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            targets = []\n",
    "            for i in range(len(batch_samples)):\n",
    "                batch_sample = batch_samples[i]\n",
    "                batch_target = batch_targets[i]\n",
    "                im = Image.open(batch_sample)\n",
    "                cur_width = im.size[0]\n",
    "                cur_height = im.size[1]\n",
    "\n",
    "                # print(cur_width, cur_height)\n",
    "                height_fac = 113 / cur_height\n",
    "\n",
    "                new_width = int(cur_width * height_fac)\n",
    "                size = new_width, 113\n",
    "\n",
    "                imresize = im.resize((size), Image.ANTIALIAS)  # Resize so height = 113 while keeping aspect ratio\n",
    "                now_width = imresize.size[0]\n",
    "                now_height = imresize.size[1]\n",
    "                # Generate crops of size 113x113 from this resized image and keep random 10% of crops\n",
    "\n",
    "                avail_x_points = list(range(0, now_width - 113 ))# total x start points are from 0 to width -113\n",
    "\n",
    "                # Pick random x%\n",
    "                pick_num = int(len(avail_x_points)*factor)\n",
    "\n",
    "                # Now pick\n",
    "                random_startx = sample(avail_x_points,  pick_num)\n",
    "\n",
    "                for start in random_startx:\n",
    "                    imcrop = imresize.crop((start, 0, start+113, 113))\n",
    "                    images.append(np.asarray(imcrop))\n",
    "                    targets.append(batch_target)\n",
    "\n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(targets)\n",
    "\n",
    "            #reshape X_train for feeding in later\n",
    "            X_train = X_train.reshape(X_train.shape[0], 113, 113, 1)\n",
    "            #convert to float and normalize\n",
    "            X_train = X_train.astype('float32')\n",
    "            X_train /= 255\n",
    "\n",
    "            #One hot encode y\n",
    "            y_train = to_categorical(y_train, num_classes)\n",
    "            yield shuffle(X_train, y_train)\n",
    "            \n",
    "train_generator = generate_data(train_files, train_targets, batch_size=batch_size, factor = 0.3)\n",
    "validation_generator = generate_data(validation_files, validation_targets, batch_size=batch_size, factor = 0.3)\n",
    "test_generator = generate_data(test_files, test_targets, batch_size=batch_size, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image):\n",
    "    import tensorflow as tf\n",
    "    return tf.image.resize(image,[56,56])\n",
    "\n",
    "# Function to resize image to 64x64\n",
    "row, col, ch = 113, 113, 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1, 1), input_shape=(row, col, ch)))\n",
    "\n",
    "# Resise data within the neural network\n",
    "model.add(Lambda(resize_image))  #resize images to allow for easy computation\n",
    "\n",
    "# CNN model - Building the model suggested in paper\n",
    "\n",
    "model.add(Convolution2D(filters= 32, kernel_size =(5,5), strides= (2,2), padding='same', name='conv1')) #96\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2), name='pool1'))\n",
    "\n",
    "model.add(Convolution2D(filters= 64, kernel_size =(3,3), strides= (1,1), padding='same', name='conv2'))  #256\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2), name='pool2'))\n",
    "\n",
    "model.add(Convolution2D(filters= 128, kernel_size =(3,3), strides= (1,1), padding='same', name='conv3'))  #256\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2), name='pool3'))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512, name='dense1'))  #1024\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256, name='dense2'))  #1024\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_classes,name='output'))\n",
    "model.add(Activation('softmax'))  #softmax since output is within 50 classes\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 10\n",
    "\n",
    "samples_per_epoch = 3268\n",
    "nb_val_samples = 842\n",
    "\n",
    "# #save every model using Keras checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#filepath=\"check-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "filepath=\"low_loss.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath= filepath, verbose=1, save_best_only=False)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# #Model fit generator\n",
    "history_object = model.fit_generator(train_generator, steps_per_epoch = samples_per_epoch/batch_size,\n",
    "                                      validation_data=validation_generator,\n",
    "                                      validation_steps=nb_val_samples, epochs=nb_epoch, verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "    FileLink(r'low_loss.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('low_loss.h5')\n",
    "scores = model.evaluate_generator(test_generator,800) \n",
    "print(\"Accuracy = \", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('low_loss.h5')\n",
    "predictions = model.predict(X_test, verbose =1)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for filename in test_files[:50]:\n",
    "     im = Image.open(filename)\n",
    "     cur_width = im.size[0]\n",
    "     cur_height = im.size[1]\n",
    "\n",
    "     print(cur_width, cur_height)\n",
    "     height_fac = 113 / cur_height\n",
    "\n",
    "     new_width = int(cur_width * height_fac)\n",
    "     size = new_width, 113\n",
    "\n",
    "     imresize = im.resize((size), Image.ANTIALIAS)  # Resize so height = 113 while keeping aspect ratio\n",
    "     now_width = imresize.size[0]\n",
    "     now_height = imresize.size[1]\n",
    "     # Generate crops of size 113x113 from this resized image and keep random 10% of crops\n",
    "\n",
    "     avail_x_points = list(range(0, now_width - 113 ))# total x start points are from 0 to width -113\n",
    "\n",
    "     # Pick random x%\n",
    "     factor = 0.1\n",
    "     pick_num = int(len(avail_x_points)*factor)\n",
    "    \n",
    "     random_startx = sample(avail_x_points,  pick_num)\n",
    "\n",
    "     for start in random_startx:\n",
    "         imcrop = imresize.crop((start, 0, start+113, 113))\n",
    "         images.append(np.asarray(imcrop))\n",
    "        \n",
    "     X_test = np.array(images)\n",
    "    \n",
    "     X_test = X_test.reshape(X_test.shape[0], 113, 113, 1)\n",
    "     #convert to float and normalize\n",
    "     X_test = X_test.astype('float32')\n",
    "     X_test /= 255\n",
    "     shuffle(X_test)\n",
    "\n",
    "     print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in img_files[:3]:\n",
    "    img=mpimg.imread(filename)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(img, cmap ='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test, verbose =1)\n",
    "print(predictions.shape)\n",
    "predicted_writer = []\n",
    "for pred in predictions:\n",
    "    predicted_writer.append(np.argmax(pred))\n",
    "print(len(predicted_writer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_number = 18\n",
    "total_images =10\n",
    "counter = 0\n",
    "for i in range(len(predicted_writer)//10):\n",
    "    if predicted_writer[i] == writer_number:\n",
    "        image = X_test[i].squeeze()\n",
    "        plt.figure(figsize=(2,2))\n",
    "        plt.imshow(image, cmap ='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
